# 1. Introduction

## Two Types of Large Language Models (LLM)

### Base LLM
Predict the next word based on large-scale text training data based on self-supervised learning.

Example: `input` output
* `Once upon a time, there was a unicorn` that lived in a magical forest with all her unicorn friends.
* `What is the capital of France?` What is France's largest city? What is France's population?

Since it is trained on public texts (like articles, blogs, etc.), it will be more likely for the model to predict/complete the texts with the same behavior as the training data.

### Instruction Tuned LLM
Tries to follow instructions. It is fine-tuned on instruction and good attempts at following those instructions.

They are commonly trained with RLHF: Reinforcement Learning with Human Feedback pipeline, so the output is aligned with human preference.

Example: `input` output
* `What is the capital of France?` The capital of France is Paris.

### Note:
The course is focus on best practices for **Instruction Tuned LLM**.
